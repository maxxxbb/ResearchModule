{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Regression (Turnover intention ~ unfair treatment x neg. reciprocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Read in SOEP Data:\n",
    "- vp : 2005 data : main variables of interest: questions on negative reciprocity\n",
    "- wp: 2006 data : main variables of interest: question on perceived recognition for work\n",
    "- xp: 2007 data : main variables of interest: turnover intentions, controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path: insert the path where the SOEP data is stored on your computer here\n",
    "from pathlib import Path\n",
    "## copy your path in here:\n",
    "# Path Max: C:/Users/max-admin/Desktop/Masterstudium/WiSe_22_23/Research_Module/SOEP-Data/Stata/raw\n",
    "# Path Maxie: /Volumes/dohmen_soep/SOEP-CORE.v36eu_STATA/Stata/raw\n",
    "\n",
    "data_folder = Path(\"C:/Users/max-admin/Desktop/Masterstudium/WiSe_22_23/Research_Module/SOEP-Data/Stata/raw\")\n",
    "# define relevant subsets of SOEP-data\n",
    "file_names = ['vp', 'wp', 'xp']\n",
    "\n",
    "file_paths = [data_folder / f\"{file_name}.dta\" for file_name in file_names]\n",
    "# some controls are in gen data\n",
    "file_paths_2 = [data_folder / f\"{file_name}gen.dta\" for file_name in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>syear</th>\n",
       "      <th>similar_problems</th>\n",
       "      <th>take_revenge</th>\n",
       "      <th>insult_back</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pid</th>\n",
       "      <th>hid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <th>27</th>\n",
       "      <td>2005</td>\n",
       "      <td>[3] Skala 1-7</td>\n",
       "      <td>[6] Skala 1-7</td>\n",
       "      <td>[4] Skala 1-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <th>60313</th>\n",
       "      <td>2005</td>\n",
       "      <td>[4] Skala 1-7</td>\n",
       "      <td>[7] Trifft voll zu</td>\n",
       "      <td>[5] Skala 1-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <th>60</th>\n",
       "      <td>2005</td>\n",
       "      <td>[5] Skala 1-7</td>\n",
       "      <td>[5] Skala 1-7</td>\n",
       "      <td>[5] Skala 1-7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           syear similar_problems        take_revenge    insult_back\n",
       "pid hid                                                             \n",
       "201 27      2005    [3] Skala 1-7       [6] Skala 1-7  [4] Skala 1-7\n",
       "203 60313   2005    [4] Skala 1-7  [7] Trifft voll zu  [5] Skala 1-7\n",
       "602 60      2005    [5] Skala 1-7       [5] Skala 1-7  [5] Skala 1-7"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in 2005 data for the reciprocity measures ##### positive not renamed yet\n",
    "data05 = pd.read_stata(file_paths[0], columns=[\"pid\",\"hid\", \"syear\",\"vp12604\", \"vp12601\", \"vp12606\"]).set_index(['pid', 'hid'])\n",
    "df_05 = data05.rename(columns={ 'vp12601': 'take_revenge', 'vp12604': 'similar_problems', 'vp12606': 'insult_back'})\n",
    "df_05.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 2006 data\n",
    "# personal = personal advancement\n",
    "# still includes all unfair treat\n",
    "data06 = pd.read_stata(file_paths[1], columns=[\"pid\", \"hid\", \"syear\", 'wp43b01', 'wp43b02', 'wp43b03', 'wp43b04', 'wp43b05', 'wp43b06', 'wp43b07','wp43b08' ]).set_index(['pid', 'hid'])\n",
    "df_06 = data06.rename(columns={ 'wp43b01': 'recog_sup', 'wp43b02': 'felt_recog_sup',\"wp43b03\": \"recog_effort\",  'wp43b04': 'felt_recog_effort', \"wp43b05\": \"recog_personal\", \"wp43b06\" :\"felt_recog_personal\" ,\"wp43b07\": \"recog_pay\",'wp43b08': 'felt_recog_pay'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in 2007 data\n",
    "# here left out 'xp8601' for school degree since we have it in another module also 'xp0102' : 'work_satisfaction' for the beginning\n",
    "#for outcome and all controls\n",
    "data3= pd.read_stata(file_paths[2], columns=[\"pid\", \"hid\", \"syear\", 'xp13101' , 'xp13102', 'xp2701', 'xp7302','xp7202','xp28', 'xp3001' ,  'xp5701' , 'xp0102']).set_index(['pid', 'hid'])\n",
    "df_07 = data3.rename(columns= {'xp13101':'gender','xp13102': 'year_birth' ,'xp2701': 'turnover_intention' , 'xp7302': 'wage_lastmonth','xp7202': 'overtime','xp28': 'new_job', 'xp3001': 'reason_new_job',  'xp5701' : 'commute_distance' , \"xp0102\" : \"work_satisfaction\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 2007 data from work module\n",
    "\n",
    "# adapt path and merge\n",
    "hours07 = pd.read_stata(file_paths_2[2], columns=[\"pid\",\"hid\", \"syear\", \"xvebzeit\", \"xpsbil\", \"betr07\", \"xerwzeit\", \"xbilzeit\"]).set_index(['pid', 'hid'])\n",
    "work07 = hours07.rename(columns={'xvebzeit': 'working_hours', \"xpsbil\": \"school_degree\", \"betr07\": \"firmsize\", \"xerwzeit\": \"tenure\" , \"xbilzeit\" : \"years_educ\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2. Define Functions and mappings for cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping for reciprocity questions: same scale for all\n",
    "reciprocity_questions_mapping = {\n",
    "    '[1] Trifft ueberhaupt nicht zu': 1,\n",
    "    '[2] Skala 1-7': 2,\n",
    "    '[3] Skala 1-7': 3,\n",
    "    '[4] Skala 1-7': 4,\n",
    "    '[5] Skala 1-7': 5,\n",
    "    '[6] Skala 1-7': 6,\n",
    "    '[7] Trifft voll zu': 7,\n",
    "    '[-1] keine Angabe': -1,\n",
    "}\n",
    "## mapping for recognition questions: binary -> binary -> unfair treatment: No -> later 1 fair treatment: Yes -> 0\n",
    "recog_mapping = {\n",
    "    '[-2] trifft nicht zu': -2,\n",
    "    '[-1] keine Angabe': -1,\n",
    "    '[1] Ja': 1,\n",
    "    '[2] Nein': 0,\n",
    "}\n",
    "# felt recog mapping\n",
    "felt_recog_mapping = {\n",
    "    '[-2] trifft nicht zu': -2,\n",
    "    '[-1] keine Angabe': -1,\n",
    "    '[1] Gar nicht': 1,\n",
    "    '[2] Maessig': 2,\n",
    "    '[3] Stark': 3,\n",
    "    '[4] Sehr stark': 4,\n",
    "}\n",
    "#spelling error\n",
    "take_revenge_mapping = {\n",
    "    '[1] Trifft ueberhautp nicht zu': 1,\n",
    "    '[2] Skala 1-7': 2,\n",
    "    '[3] Skala 1-7': 3,\n",
    "    '[4] Skala 1-7': 4,\n",
    "    '[5] Skala 1-7': 5,\n",
    "    '[6] Skala 1-7': 6,\n",
    "    '[7] Trifft voll zu': 7,\n",
    "    '[-1] keine Angabe': -1,\n",
    "}\n",
    "\n",
    "# mapping for firmsize -> we need to recode this in a sensible way: jumps are the same: first change: selbstständig to 0\n",
    "firmsize_mapping = {\n",
    "    '[-2] trifft nicht zu': -2,\n",
    "    '[-1] keine Angabe': -1,\n",
    "    '[1] Unter  5': 1,\n",
    "    '[2] 5 bis 10': 2,\n",
    "    '[3] 11 bis unter 20': 3,\n",
    "    '[4] bis 90: unter 20': 4,\n",
    "    '[5] 91-04: 5 bis unter 20': 5,\n",
    "    '[6] 20 bis unter 100': 6,\n",
    "    '[7] 100 bis unter 200': 7,\n",
    "    '[8] bis 98: 20 bis unter 200': 8,\n",
    "    '[9] 200 bis unter 2000': 9,\n",
    "    '[10] 2000 und mehr': 10,\n",
    "    '[11] Selbstaendig-ohne Mitarb.': 0,\n",
    "}\n",
    "# mapping new job into binary variable\n",
    "new_job_mapping = {\n",
    "    '[-2] trifft nicht zu': -2,\n",
    "    '[-1] keine Angabe': -1, \n",
    "    '[1] Ja': 1, \n",
    "    '[2] Nein': 2,\n",
    "    '[3] Ja, nach Datenpruefung': 1,\n",
    "}\n",
    "# mapping for job satisfaction: split up into binary with roughly equal value counts for simplicity: might change that later to categories,\n",
    "# vielleicht mapping nochmal ändern ? \n",
    "satisfaction_mapping = {\n",
    "    '[0] 0 Zufrieden: Skala 0-Niedrig bis 10-Hoch': 2,\n",
    "    '[1] 1 Zufrieden: Skala 0-Niedrig bis 10-Hoch': 2,\n",
    "    '[2] 2 Zufrieden: Skala 0-Niedrig bis 10-Hoch': 2,\n",
    "    '[3] 3 Zufrieden: Skala 0-Niedrig bis 10-Hoch': 2,\n",
    "    '[4] 4 Zufrieden: Skala 0-Niedrig bis 10-Hoch': 2,\n",
    "    '[5] 5 Zufrieden: Skala 0-Niedrig bis 10-Hoch': 2,\n",
    "    '[6] 6 Zufrieden: Skala 0-Niedrig bis 10-Hoch': 1,\n",
    "    '[7] 7 Zufrieden: Skala 0-Niedrig bis 10-Hoch': 1,\n",
    "    '[8] 8 Zufrieden: Skala 0-Niedrig bis 10-Hoch': 1,\n",
    "    '[9] 9 Zufrieden: Skala 0-Niedrig bis 10-Hoch': 1,\n",
    "    '[10] 10 Zufrieden: Skala 0-Niedrig bis 10-Hoc': 1,\n",
    "    '[-2] trifft nicht zu': -2,\n",
    "    '[-1] keine Angabe': -1,\n",
    "}\n",
    "turnover_mapping = {\n",
    "    '[-2] trifft nicht zu': -2,\n",
    "    '[-1] keine Angabe':-1,\n",
    "    '[0] 0% wahrscheinlich': 2,\n",
    "    '[10] 10% wahrscheinlich': 1,\n",
    "    '[20] 20% wahrscheinlich': 1,\n",
    "    '[30] 30% wahrscheinlich': 1,\n",
    "    '[40] 40% wahrscheinlich': 1, \n",
    "    '[50] 50% wahrscheinlich': 1,\n",
    "    '[60] 60% wahrscheinlich': 1,\n",
    "    '[70] 70% wahrscheinlich': 1,\n",
    "    '[80] 80% wahrscheinlich': 1,\n",
    "    '[90] 90% wahrscheinlich': 1,\n",
    "    '[100] 100% wahrscheinlich': 1,\n",
    "}\n",
    "# mapping for turnover intention robustness check: Cardinal\n",
    "turnover_mapping_cardinal = {\n",
    "    '[-2] trifft nicht zu': -2,\n",
    "    '[-1] keine Angabe':-1,\n",
    "    '[0] 0% wahrscheinlich': 0,\n",
    "    '[10] 10% wahrscheinlich': 10,\n",
    "    '[20] 20% wahrscheinlich': 20,\n",
    "    '[30] 30% wahrscheinlich': 30,\n",
    "    '[40] 40% wahrscheinlich': 40, \n",
    "    '[50] 50% wahrscheinlich': 50,\n",
    "    '[60] 60% wahrscheinlich': 60,\n",
    "    '[70] 70% wahrscheinlich': 70,\n",
    "    '[80] 80% wahrscheinlich': 80,\n",
    "    '[90] 90% wahrscheinlich': 90,\n",
    "    '[100] 100% wahrscheinlich': 100,\n",
    "}\n",
    "# mapping for new job to easier remove negatives\n",
    "reason_new_job_mapping = {\n",
    "    '[-2] trifft nicht zu': -2,\n",
    "    '[-1] keine Angabe':-1, \n",
    "    '[1] Erstmals erwerbstaetig': 1,\n",
    "    '[2] Wieder erwerbstaetig': 2,\n",
    "    '[3] Stelle bei neuen Arbeitgeber': 3,  \n",
    "    '[4] Uerbnommen von Betrieb': 4,\n",
    "    '[5] Stellenwechsel im Betrieb': 5, \n",
    "    '[6] Selbstaendig geworden': 6,\n",
    "}\n",
    "# mapping for school degree: to easier remove negatives\n",
    "school_degree_mapping = {\n",
    "    '[-2] trifft nicht zu': -2,\n",
    "    '[-1] keine Angabe':-1,\n",
    "    '[1] Hauptschulabschluss': 1,\n",
    "    '[2] Realschulabschluss': 2,\n",
    "    '[3] Fachhochschulreife': 3,\n",
    "    '[4] Abitur': 4,\n",
    "    '[5] Anderer Abschluss': 5,\n",
    "    '[6] Ohne Abschluss verlassen': 6,\n",
    "    '[7] Noch kein Abschluss': 7,\n",
    "    '[8] Keine Schule besucht': 8,\n",
    "}\n",
    "# reversed mapping to redo changes\n",
    "reversed_mapping_reason = {v: k for k, v in reason_new_job_mapping.items()}\n",
    "reversed_mapping_schoold = {v: k for k, v in school_degree_mapping.items()}\n",
    "\n",
    "## function for recoding values and dropping missing\n",
    "\n",
    "def recode_categoricals(inputdf):\n",
    "    \"\"\"\n",
    "        merges data from different years, applies recoding to categoricals and constructs additional variables.\n",
    "\n",
    "        Input:\n",
    "            - inputdf : merged dataframe\n",
    "            - rc_cardinal: optional argument: if == 1 turnover intentions will be coded as cardinal instead of binary \n",
    "            - rc_rec_binary: optional argument: if == 1 neg. reciprocity will be split into 2 groups: high and low\n",
    "        Output:\n",
    "            - df : cleaned Dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    merged = inputdf  \n",
    "    \n",
    "    # recode Gender variable\n",
    "    merged['gender'].replace('[2] Weiblich', 2,inplace=True)\n",
    "    merged['gender'].replace('[1] Maennlich', 1,inplace=True)\n",
    "    # recode reciprocity variables\n",
    "    merged[[\"similar_problems\",\"insult_back\"]] = merged[[\"similar_problems\",\"insult_back\"]].apply(lambda x: x.map(reciprocity_questions_mapping))\n",
    "    merged[\"take_revenge\"] = merged[\"take_revenge\"].map(take_revenge_mapping)\n",
    "    # recode recognition variables\n",
    "    merged[[\"recog_sup\",\"recog_effort\",\"recog_personal\",\"recog_pay\"]] = merged[[\"recog_sup\",\"recog_effort\",\"recog_personal\",\"recog_pay\"]].apply(lambda x: x.map(recog_mapping))\n",
    "    # recode felt recognition variables\n",
    "    merged[[\"felt_recog_sup\",\"felt_recog_effort\",\"felt_recog_personal\",\"felt_recog_pay\"]] = merged[[\"felt_recog_sup\",\"felt_recog_effort\",\"felt_recog_personal\",\"felt_recog_pay\"]].apply(lambda x: x.map(felt_recog_mapping))\n",
    "    # recode firm size\n",
    "    merged['firmsize'] = merged['firmsize'].map(firmsize_mapping)\n",
    "    # recode new job reason variable\n",
    "    merged['reason_new_job'] = merged['reason_new_job'].map(reason_new_job_mapping)\n",
    "    # recode job change variable\n",
    "    merged['new_job']= merged['new_job'].map(new_job_mapping)\n",
    "    # recode job satisfaction\n",
    "    merged['work_satisfaction']= merged['work_satisfaction'].map(satisfaction_mapping)\n",
    "    # recode turnover intention variable\n",
    "    merged['turnover_intention'] = merged['turnover_intention'].map(turnover_mapping)\n",
    "    # recode school degree\n",
    "    merged['school_degree'] = merged['school_degree'].map(school_degree_mapping)\n",
    "    \n",
    "    output = merged\n",
    "    return output\n",
    "\n",
    "# Merge dataframes: a bit tough to read as its nested, merges 4 dataframes: 2005,2006,2007 and 2007gen\n",
    "\n",
    "def merge_and_clean(df_05,df_06,df_07,work07):\n",
    "    \"\"\"\n",
    "    merges data from different years, applies recoding to categoricals and constructs additional variables.\n",
    "\n",
    "    Input:\n",
    "        - df_05 : Pd.Dataframe contains reciprocity measures\n",
    "        - df_06 : Pd.Dataframe contains unfair treatment measures\n",
    "        - df_07 : Pd.Dataframe contains outcome and controls\n",
    "        - work07 : Pd.Dataframe contains additional controls\n",
    "\n",
    "    Output:\n",
    "        - df : cleaned Dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    allmerged_df = pd.merge(pd.merge(pd.merge(df_05,df_06,on=[\"pid\", \"hid\"]),work07,on=[\"pid\",\"hid\"]),df_07,on=[\"pid\", \"hid\"])\n",
    "    recoded = recode_categoricals(allmerged_df).astype('int')\n",
    "\n",
    "    # replaces negative values with n.a.n \n",
    "    recoded = recoded.mask(recoded < 0, np.nan) \n",
    "    # construct avg reciprocity measure\n",
    "    recoded['avg_rec'] = recoded[['take_revenge', 'similar_problems', 'insult_back']].mean(axis=1)\n",
    "    # For robustness check: option to construct binary reciprocity measure.\n",
    "    \n",
    "    # construct age, potential experience and age^2\n",
    "    recoded['age'] = 2007 - recoded['year_birth']\n",
    "    recoded[\"potential_experience\"] = pow((recoded[\"age\"] - 18), 2)\n",
    "    recoded[\"age_squared\"] = (recoded[\"age\"] ** 2) / 100\n",
    "    # recode categoricals back to make it better readable\n",
    "    recoded[\"reason_new_job\"] = recoded[\"reason_new_job\"].map(reversed_mapping_reason)\n",
    "    recoded[\"school_degree\"] = recoded[\"school_degree\"].map(reversed_mapping_schoold)\n",
    "\n",
    "    # transform binary variables with 1 and 2 into 1 and 0\n",
    "    columns_to_transform = [\"recog_sup\",\"recog_effort\", \"recog_pay\", \"recog_personal\" ,\"gender\", \"turnover_intention\", \"new_job\"]\n",
    "\n",
    "    # Iterate over the columns and replace the values 2 with 0 \n",
    "    for col in columns_to_transform:\n",
    "        recoded[col] = recoded[col].replace({2: 0})\n",
    "\n",
    "    # save df somewhere so its not muted when repeatedly executing this cell: Can later transform that into functions\n",
    "    df = recoded\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add Mincer Wage Regression and adds its residuals to the Dataframe\n",
    "\n",
    "def add_mincer_residuals(cleaneddata):\n",
    "    \n",
    "    df_cleaned = cleaneddata\n",
    "    # specify which columns to drop from our dataframe\n",
    "    df_mincer = cleaneddata.drop(columns=['syear_x', 'similar_problems', 'take_revenge', 'insult_back','syear_y', 'felt_recog_sup', 'felt_recog_effort',\n",
    "       'recog_personal', 'felt_recog_personal', 'recog_pay', 'felt_recog_pay', 'syear_y', 'year_birth', 'new_job',\n",
    "       'reason_new_job', 'school_degree','overtime', 'recog_sup', 'age', 'commute_distance', 'recog_effort', 'working_hours', 'turnover_intention', 'work_satisfaction'])\n",
    "    # Convert 'gender' and 'sector' columns to categorical data type\n",
    "    for col in ['gender']:\n",
    "        df_mincer[col] = df_mincer[col].astype('category')\n",
    "    df_mincer = df_mincer.dropna()\n",
    "    # Define the dependent variable\n",
    "    y = df_mincer['wage_lastmonth']\n",
    "    # Define the independent variables\n",
    "    X = df_mincer[['gender', 'firmsize', 'tenure', 'years_educ', 'potential_experience', 'age_squared']]\n",
    "\n",
    "    # Add a constant term to the independent variables\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Fit the Mincer wage regression model\n",
    "    mincer_model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # Create a new column in the dataframe with the same name as the residuals array/ delete relative wage entries\n",
    "    df_cleaned['mincer_residuals'] = None\n",
    "    # Match the rows of the dataframe with the values in the residuals array using the index\n",
    "    df_cleaned.loc[df_cleaned.index, 'mincer_residuals'] = mincer_model.resid\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inlcude People who switched their jobs in 2006-2007 with 2006 controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_jobchangers(data07,onlynewemployer=True):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "            df: finished 2007 dataframe\n",
    "            onlynewemployer: optional argument: If True only includes those who switched jobs to a new employer\n",
    "            worksatisfaction: if True drops observations where job satisfaction between 2006 and 2007 changed to check for our Assumption\n",
    "    \n",
    "    \"\"\"\n",
    "    df = data07\n",
    "    # read in cleaned 2006 dataset:\n",
    "    #  change path here : Maxie: '/Users/maxieschulze/Documents/Dokumente - MacBook Pro von Maxie/5. Semester/Research Module/ResearchModule/src/data_management/2006jobchange.csv'\n",
    "    #                     Max 'C:/Users/max-admin/Desktop/Masterstudium/WiSe_22_23/Research_Module/finalproj/src/data_management/2006jobchange.csv'  \n",
    "    observations_2006 = pd.read_csv('C:/Users/max-admin/Desktop/Masterstudium/WiSe_22_23/Research_Module/finalproj/src/data_management/2006jobchangepositiverec.csv')\n",
    "    # some initial datamanagement\n",
    "    observations_2006.reset_index(inplace=True)\n",
    "    observations_2006.set_index([\"pid\",\"hid\"], inplace=True)\n",
    "    observations_2006.drop(columns=observations_2006.filter(regex='^syear').columns, inplace=True)\n",
    "    observations_2006.drop(\"index\",axis=1 ,inplace =True)\n",
    "    # If cardinal turnover intentions are coded to 100\n",
    "    # drop s year columns from both dataframes\n",
    "    df.drop(columns=df.filter(regex='^syear').columns, inplace=True)\n",
    "    \n",
    "    # create list of IDs of people who switched to a new employer in the last year in 2007\n",
    "    if onlynewemployer == True:\n",
    "        IDs_tokeep = df[(df[\"new_job\"] == 1) & (df[\"reason_new_job\"] == '[3] Stelle bei neuen Arbeitgeber')].index\n",
    "    # if True drops people whose work satisfaction changed between 2006 and 2007\n",
    "    else:\n",
    "        IDs_tokeep = df[(df[\"new_job\"] == 1)].index\n",
    "    \n",
    "    # drop all who changed their job in 2007 dataframe and replace 2007 controls with 2006 controls\n",
    "    df.drop(df[df[\"new_job\"] == 1].index, inplace = True)\n",
    "    observations_2006.drop(observations_2006[~observations_2006.index.isin(IDs_tokeep)].index, inplace=True)\n",
    "    #concat both dataframes\n",
    "    dfconcat = pd.concat([df,observations_2006])\n",
    "    \n",
    "    return dfconcat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analysis\n",
    "#### 1.Probit Regression\n",
    "\n",
    "- Probit Estimation: TurnoverIntention_{2005} = Constant + Neg-Rec + Unfair + Rec X Unfair + Controls + Error\n",
    "\n",
    "- As the measure for unfair treatment in the first regression i first used recog_effort -> decide for one later\n",
    "    - \"When I consider all my accomplishments and efforts, the recognition of I've received seems about right to me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4563: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n",
      "<ipython-input-51-047203a7ccc4>:238: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mincer[col] = df_mincer[col].astype('category')\n"
     ]
    }
   ],
   "source": [
    "df_OLS = add_mincer_residuals(merge_and_clean(df_05,df_06,df_07,work07))\n",
    "df_OLS.drop(columns=df_OLS.filter(regex='^syear').columns, inplace=True)\n",
    "df_OLS.drop(df_OLS[df_OLS[\"new_job\"] == 1].index, inplace = True)\n",
    "# specify columns which we need for regression by dropping everything else\n",
    "df_OLS = df_OLS.drop(columns=['similar_problems', 'take_revenge', 'insult_back', 'felt_recog_sup', 'felt_recog_effort',\n",
    "       'recog_personal', 'felt_recog_personal', 'recog_pay', 'felt_recog_pay', 'year_birth', 'new_job',\n",
    "       'reason_new_job', 'school_degree','overtime', 'recog_sup','wage_lastmonth','work_satisfaction'])\n",
    "#  included 'recog_effort'again , excluded overtime due to sample size and wage last month\n",
    "# add interaction term\n",
    "df_OLS[\"recXrecog_effort\"] = df_OLS[\"recog_effort\"] * df_OLS[\"avg_rec\"]\n",
    "# drop missing data in regression dataframe\n",
    "df_OLS.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.592775\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>turnover_intention</td> <th>  No. Observations:  </th>   <td>  5409</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Probit</td>       <th>  Df Residuals:      </th>   <td>  5395</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>MLE</td>        <th>  Df Model:          </th>   <td>    13</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Jan 2023</td>  <th>  Pseudo R-squ.:     </th>   <td>0.1444</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:55:47</td>      <th>  Log-Likelihood:    </th>  <td> -3206.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>              <td>False</td>       <th>  LL-Null:           </th>  <td> -3747.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>  LLR p-value:       </th> <td>3.202e-223</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>              <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                <td>    0.0592</td> <td> 4.04e+05</td> <td> 1.46e-07</td> <td> 1.000</td> <td>-7.93e+05</td> <td> 7.93e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>recog_effort</th>         <td>   -0.2864</td> <td>    0.257</td> <td>   -1.116</td> <td> 0.264</td> <td>   -0.789</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>working_hours</th>        <td>   -0.0019</td> <td>    0.003</td> <td>   -0.745</td> <td> 0.456</td> <td>   -0.007</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>firmsize</th>             <td>   -0.0353</td> <td>    0.007</td> <td>   -5.387</td> <td> 0.000</td> <td>   -0.048</td> <td>   -0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tenure</th>               <td>   -0.0262</td> <td>    0.002</td> <td>  -11.002</td> <td> 0.000</td> <td>   -0.031</td> <td>   -0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>years_educ</th>           <td>    0.0565</td> <td>    0.007</td> <td>    8.216</td> <td> 0.000</td> <td>    0.043</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender</th>               <td>    0.1617</td> <td>    0.043</td> <td>    3.751</td> <td> 0.000</td> <td>    0.077</td> <td>    0.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>commute_distance</th>     <td>    0.0012</td> <td>    0.000</td> <td>    2.924</td> <td> 0.003</td> <td>    0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_rec</th>              <td>   -0.0819</td> <td>    0.034</td> <td>   -2.391</td> <td> 0.017</td> <td>   -0.149</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>                  <td>    0.0762</td> <td> 4.49e+04</td> <td>  1.7e-06</td> <td> 1.000</td> <td>-8.81e+04</td> <td> 8.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>potential_experience</th> <td>    0.0006</td> <td> 1248.070</td> <td> 4.57e-07</td> <td> 1.000</td> <td>-2446.171</td> <td> 2446.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age_squared</th>          <td>   -0.1649</td> <td> 1.25e+05</td> <td>-1.32e-06</td> <td> 1.000</td> <td>-2.45e+05</td> <td> 2.45e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mincer_residuals</th>     <td>-4.271e-05</td> <td> 2.79e-05</td> <td>   -1.530</td> <td> 0.126</td> <td>-9.74e-05</td> <td>  1.2e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>recXrecog_effort</th>     <td>   -0.0075</td> <td>    0.043</td> <td>   -0.175</td> <td> 0.861</td> <td>   -0.092</td> <td>    0.077</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Probit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:     turnover_intention   No. Observations:                 5409\n",
       "Model:                         Probit   Df Residuals:                     5395\n",
       "Method:                           MLE   Df Model:                           13\n",
       "Date:                Wed, 11 Jan 2023   Pseudo R-squ.:                  0.1444\n",
       "Time:                        20:55:47   Log-Likelihood:                -3206.3\n",
       "converged:                      False   LL-Null:                       -3747.6\n",
       "Covariance Type:            nonrobust   LLR p-value:                3.202e-223\n",
       "========================================================================================\n",
       "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------\n",
       "const                    0.0592   4.04e+05   1.46e-07      1.000   -7.93e+05    7.93e+05\n",
       "recog_effort            -0.2864      0.257     -1.116      0.264      -0.789       0.217\n",
       "working_hours           -0.0019      0.003     -0.745      0.456      -0.007       0.003\n",
       "firmsize                -0.0353      0.007     -5.387      0.000      -0.048      -0.022\n",
       "tenure                  -0.0262      0.002    -11.002      0.000      -0.031      -0.022\n",
       "years_educ               0.0565      0.007      8.216      0.000       0.043       0.070\n",
       "gender                   0.1617      0.043      3.751      0.000       0.077       0.246\n",
       "commute_distance         0.0012      0.000      2.924      0.003       0.000       0.002\n",
       "avg_rec                 -0.0819      0.034     -2.391      0.017      -0.149      -0.015\n",
       "age                      0.0762   4.49e+04    1.7e-06      1.000   -8.81e+04    8.81e+04\n",
       "potential_experience     0.0006   1248.070   4.57e-07      1.000   -2446.171    2446.172\n",
       "age_squared             -0.1649   1.25e+05  -1.32e-06      1.000   -2.45e+05    2.45e+05\n",
       "mincer_residuals     -4.271e-05   2.79e-05     -1.530      0.126   -9.74e-05     1.2e-05\n",
       "recXrecog_effort        -0.0075      0.043     -0.175      0.861      -0.092       0.077\n",
       "========================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.discrete.discrete_model import Probit\n",
    "# Define Outcome variable\n",
    "Y = df_OLS[\"turnover_intention\"]\n",
    "# define X matrix\n",
    "X = df_OLS.drop(columns=[\"turnover_intention\"])\n",
    "# add constant \n",
    "X = sm.add_constant(X)\n",
    "# build model\n",
    "model = Probit(Y, X.astype(float))\n",
    "# estimate model\n",
    "probit_model = model.fit()\n",
    "# print summary table\n",
    "probit_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. OLS\n",
    "Now, instead of using a probit model, we use OLS.  linear probability model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>turnover_intention</td> <th>  R-squared:         </th> <td>   0.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                    <td>OLS</td>        <th>  Adj. R-squared:    </th> <td>   0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>              <td>Least Squares</td>   <th>  F-statistic:       </th> <td>   577.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>              <td>Wed, 11 Jan 2023</td>  <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                  <td>20:55:48</td>      <th>  Log-Likelihood:    </th> <td> -3384.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>       <td>  5409</td>       <th>  AIC:               </th> <td>   6795.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>           <td>  5396</td>       <th>  BIC:               </th> <td>   6881.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>               <td>    12</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>          <td>HC3</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>              <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                <td>    0.0805</td> <td>    0.010</td> <td>    8.073</td> <td> 0.000</td> <td>    0.061</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>recog_effort</th>         <td>   -0.1032</td> <td>    0.088</td> <td>   -1.176</td> <td> 0.240</td> <td>   -0.275</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>working_hours</th>        <td>   -0.0009</td> <td>    0.001</td> <td>   -1.038</td> <td> 0.299</td> <td>   -0.003</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>firmsize</th>             <td>   -0.0120</td> <td>    0.002</td> <td>   -5.304</td> <td> 0.000</td> <td>   -0.016</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tenure</th>               <td>   -0.0086</td> <td>    0.001</td> <td>  -11.366</td> <td> 0.000</td> <td>   -0.010</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>years_educ</th>           <td>    0.0195</td> <td>    0.002</td> <td>    8.875</td> <td> 0.000</td> <td>    0.015</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender</th>               <td>    0.0576</td> <td>    0.014</td> <td>    3.986</td> <td> 0.000</td> <td>    0.029</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>commute_distance</th>     <td>    0.0004</td> <td>    0.000</td> <td>    3.214</td> <td> 0.001</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_rec</th>              <td>   -0.0285</td> <td>    0.012</td> <td>   -2.405</td> <td> 0.016</td> <td>   -0.052</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>                  <td>    0.0847</td> <td>    0.009</td> <td>    9.243</td> <td> 0.000</td> <td>    0.067</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>potential_experience</th> <td>    0.0020</td> <td>    0.000</td> <td>    6.319</td> <td> 0.000</td> <td>    0.001</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age_squared</th>          <td>   -0.2302</td> <td>    0.029</td> <td>   -7.924</td> <td> 0.000</td> <td>   -0.287</td> <td>   -0.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mincer_residuals</th>     <td>-1.331e-05</td> <td>  9.4e-06</td> <td>   -1.416</td> <td> 0.157</td> <td>-3.17e-05</td> <td> 5.12e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>recXrecog_effort</th>     <td>   -0.0017</td> <td>    0.015</td> <td>   -0.114</td> <td> 0.909</td> <td>   -0.031</td> <td>    0.027</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>119566.035</td> <th>  Durbin-Watson:     </th> <td>   1.829</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td> 413.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td>-0.039</td>   <th>  Prob(JB):          </th> <td>1.67e-90</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>        <td> 1.648</td>   <th>  Cond. No.          </th> <td>6.64e+17</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors are heteroscedasticity robust (HC3)<br/>[2] The smallest eigenvalue is 1.12e-26. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:     turnover_intention   R-squared:                       0.181\n",
       "Model:                            OLS   Adj. R-squared:                  0.179\n",
       "Method:                 Least Squares   F-statistic:                     577.7\n",
       "Date:                Wed, 11 Jan 2023   Prob (F-statistic):               0.00\n",
       "Time:                        20:55:48   Log-Likelihood:                -3384.5\n",
       "No. Observations:                5409   AIC:                             6795.\n",
       "Df Residuals:                    5396   BIC:                             6881.\n",
       "Df Model:                          12                                         \n",
       "Covariance Type:                  HC3                                         \n",
       "========================================================================================\n",
       "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------\n",
       "const                    0.0805      0.010      8.073      0.000       0.061       0.100\n",
       "recog_effort            -0.1032      0.088     -1.176      0.240      -0.275       0.069\n",
       "working_hours           -0.0009      0.001     -1.038      0.299      -0.003       0.001\n",
       "firmsize                -0.0120      0.002     -5.304      0.000      -0.016      -0.008\n",
       "tenure                  -0.0086      0.001    -11.366      0.000      -0.010      -0.007\n",
       "years_educ               0.0195      0.002      8.875      0.000       0.015       0.024\n",
       "gender                   0.0576      0.014      3.986      0.000       0.029       0.086\n",
       "commute_distance         0.0004      0.000      3.214      0.001       0.000       0.001\n",
       "avg_rec                 -0.0285      0.012     -2.405      0.016      -0.052      -0.005\n",
       "age                      0.0847      0.009      9.243      0.000       0.067       0.103\n",
       "potential_experience     0.0020      0.000      6.319      0.000       0.001       0.003\n",
       "age_squared             -0.2302      0.029     -7.924      0.000      -0.287      -0.173\n",
       "mincer_residuals     -1.331e-05    9.4e-06     -1.416      0.157   -3.17e-05    5.12e-06\n",
       "recXrecog_effort        -0.0017      0.015     -0.114      0.909      -0.031       0.027\n",
       "==============================================================================\n",
       "Omnibus:                   119566.035   Durbin-Watson:                   1.829\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              413.435\n",
       "Skew:                          -0.039   Prob(JB):                     1.67e-90\n",
       "Kurtosis:                       1.648   Cond. No.                     6.64e+17\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC3)\n",
       "[2] The smallest eigenvalue is 1.12e-26. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.regression.linear_model import OLS\n",
    "model = OLS(Y, X.astype(float))\n",
    "lpm_model = model.fit(cov_type= \"HC3\")\n",
    "lpm_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aeae005699da9ee22f5f1999a0228188e87e61d9f977527a9e2e028c3963d1b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
